# ğŸš€ AI Log Insights Dashboard

Production-ready, end-to-end system to ingest logs, index/search in Elasticsearch, run LLM-based analysis and summaries via LangChain, and explore insights in a Streamlit dashboard.

## âœ¨ Features

- ğŸ“¨ Ingest logs via REST or file upload
- ğŸ” Index logs into Elasticsearch with structured fields
- ğŸ§  Semantic search via embeddings (OpenAI or local SentenceTransformers)
- ğŸ•µï¸ Summaries, anomaly/error detection via LangChain chains
- ğŸ–¥ï¸ Streamlit dashboard for exploration, QA and reports
- ğŸ§ª CI (GitHub Actions), unit tests, Dockerized deployment

## ğŸ§° Tech stack

- Python 3.10+
- FastAPI, Uvicorn
- LangChain
- Elasticsearch Python client
- SentenceTransformers (local embeddings fallback)
- Streamlit
- Docker / docker-compose
- pytest

## ğŸ“¦ Project layout

```
ai-log-insights/
â”œâ”€ README.md
â”œâ”€ LICENSE
â”œâ”€ requirements.txt
â”œâ”€ docker-compose.yml
â”œâ”€ docker-compose.prod.yml
â”œâ”€ .env.example
â”œâ”€ backend/
â”‚  â”œâ”€ app/
â”‚  â”‚  â”œâ”€ main.py
â”‚  â”‚  â”œâ”€ routers/
â”‚  â”‚  â”‚  â”œâ”€ logs.py
â”‚  â”‚  â”‚  â”œâ”€ bulk_ingest.py
â”‚  â”‚  â”‚  â””â”€ auth.py
â”‚  â”‚  â”œâ”€ services/
â”‚  â”‚  â”‚  â”œâ”€ elasticsearch_client.py
â”‚  â”‚  â”‚  â”œâ”€ embeddings.py
â”‚  â”‚  â”‚  â”œâ”€ embeddings_switch.py
â”‚  â”‚  â”‚  â”œâ”€ langchain_agent.py
â”‚  â”‚  â”‚  â”œâ”€ langchain_chains.py
â”‚  â”‚  â”‚  â””â”€ auth.py
â”‚  â”‚  â”œâ”€ models/schemas.py
â”‚  â”‚  â”œâ”€ utils/logger.py
â”‚  â”‚  â””â”€ worker/scheduler.py
â”‚  â””â”€ Dockerfile
â”œâ”€ streamlit_app/
â”‚  â”œâ”€ app.py
â”‚  â”œâ”€ components/ui.py
â”‚  â””â”€ Dockerfile
â”œâ”€ scripts/
â”‚  â”œâ”€ generate_sample_logs.py
â”‚  â””â”€ ingest_sample.sh
â”œâ”€ tests/
â”‚  â”œâ”€ test_elasticsearch.py
â”‚  â””â”€ test_api.py
â”œâ”€ .github/
â”‚  â”œâ”€ workflows/ci.yml
â”‚  â”œâ”€ ISSUE_TEMPLATE/
â”‚  â”‚  â”œâ”€ bug_report.md
â”‚  â”‚  â””â”€ feature_request.md
â”‚  â”œâ”€ PULL_REQUEST_TEMPLATE.md
â”‚  â””â”€ FUNDING.yml
â”œâ”€ docs/
â”‚  â”œâ”€ production-deploy.md
â”‚  â””â”€ auth-design.md
â”œâ”€ kibana/
â”‚  â”œâ”€ dashboards/logs_saved_objects.json
â”‚  â””â”€ import_dashboards.sh
â”œâ”€ SUPPORT.md
â”œâ”€ CONTRIBUTING.md
â”œâ”€ CODE_OF_CONDUCT.md
â”œâ”€ SECURITY.md
â”œâ”€ .flake8
â”œâ”€ .pre-commit-config.yaml
â”œâ”€ .dockerignore
â”œâ”€ .editorconfig
â””â”€ Makefile
```

## âš¡ Quickstart (Docker)

1. Copy `.env.example` to `.env` and set values (set `USE_LOCAL_EMBEDDINGS=true` to avoid OpenAI).
2. Start services:

```bash
docker compose up --build
```

3. Open:
- Backend API: `http://localhost:8000`
- Streamlit: `http://localhost:8501`
- Elasticsearch: `http://localhost:9200`

4. Seed sample data:

```bash
make ingest
```

5. Try a semantic search:

```bash
curl "http://localhost:8000/api/logs/search?q=timeout&k=3"
```

## ğŸ› ï¸ Development (no Docker)

```bash
python -m venv .venv
. .venv/bin/activate  # on Windows: .venv\Scripts\activate
pip install -r requirements.txt
uvicorn backend.app.main:app --reload --port 8000
```

Streamlit:

```bash
cd streamlit_app
export BACKEND_URL=http://localhost:8000  # on Windows: set BACKEND_URL=http://localhost:8000
streamlit run app.py --server.port 8501
```

## âœ… Tests

```bash
pytest -q
```

## âš™ï¸ Environment variables

You can set these in `.env`:

- ELASTIC_HOST, ELASTIC_PORT, ELASTIC_INDEX (default: `logs`)
- USE_LOCAL_EMBEDDINGS (`true`|`false`)
- LOCAL_EMBEDDINGS_MODEL (default: `all-MiniLM-L6-v2`)
- OPENAI_API_KEY (when using OpenAI)
- EMBED_PROVIDER (`local`|`openai`|`pinecone`)
- PINECONE_API_KEY, PINECONE_ENV, PINECONE_INDEX
- BACKEND_HOST, BACKEND_PORT
- STREAMLIT_PORT, BACKEND_URL (Streamlit to backend)
- JWT_SECRET, AUTH_DB_PATH

## ğŸ“¡ API reference (selected)

- GET `/` â†’ service health
- POST `/api/logs/ingest` â†’ ingest single log
- POST `/api/logs/analyze` â†’ analyze a batch (LLM)
- GET `/api/logs/search?q=...&k=5` â†’ semantic search
- POST `/api/auth/signup` â†’ create user
- POST `/api/auth/token` â†’ OAuth2 password â†’ `access_token`
- POST `/api/logs/bulk` (Bearer token) â†’ bulk ingest

Examples:

```bash
# single ingest
curl -X POST http://localhost:8000/api/logs/ingest \
  -H 'Content-Type: application/json' \
  -d '{"timestamp":"2025-10-08T12:00:00Z","level":"ERROR","source":"api","message":"timeout calling db"}'

# get token
curl -X POST http://localhost:8000/api/auth/token \
  -H 'Content-Type: application/x-www-form-urlencoded' \
  -d 'username=alice&password=secret'

# bulk ingest (replace TOKEN)
curl -X POST http://localhost:8000/api/logs/bulk \
  -H 'Authorization: Bearer TOKEN' -H 'Content-Type: application/json' \
  -d '[{"level":"INFO","message":"hello"},{"level":"ERROR","message":"failed"}]'
```

## ğŸ”Œ Embedding providers

- `local` (default): SentenceTransformers, fast and free. Ensure ES mapping `dims` matches the model (384 for MiniLM).
- `openai`: set `OPENAI_API_KEY`. Cost applies.
- `pinecone`: vectors are also upserted to Pinecone; keep ES mapping consistent for local similarity too.

Switch provider via `EMBED_PROVIDER`.

## â° Scheduler (daily summary)

Background scheduler is available in `backend/app/worker/scheduler.py`.
To enable in the API process, add:

```python
# in backend/app/main.py
from worker.scheduler import start_scheduler
if os.getenv('ENABLE_SCHEDULER','false').lower() in ('true','1'):
    start_scheduler()
```

Then set `ENABLE_SCHEDULER=true` in `.env`.

## ğŸ§© Troubleshooting

- âŒ `docker compose` not found
  - Install Docker Desktop and ensure `docker` works in your shell.
- â³ Elasticsearch not ready / connection refused
  - First boot can take 30â€“60s. Retry once healthy: `curl http://localhost:9200`.
- ğŸ“ Vector dims mismatch
  - Ensure ES index mapping `embedding.dims` matches your model dimension.
- ğŸ”‘ OpenAI errors (401/429)
  - Check `OPENAI_API_KEY`, rate limits, and consider `USE_LOCAL_EMBEDDINGS=true`.
- ğŸ›¡ï¸ Auth errors (401)
  - Obtain token via `/api/auth/token` and use `Authorization: Bearer <token>`.

## â“ FAQ

- Can I use Elastic Cloud? â†’ Yes. Set `ELASTIC_HOST`, `ELASTIC_PORT`, and credentials.
- How big can logs be? â†’ This starter is for demos; for production, add batching and backpressure (see bulk endpoint).
- How do I change the model? â†’ Set `LOCAL_EMBEDDINGS_MODEL` and update ES dims.

## âš™ï¸ Performance tips

- Use `/api/logs/bulk` with batches of 500â€“1000 documents.
- Disable Streamlit in ingest-only environments.
- Scale backend replicas behind Traefik in `docker-compose.prod.yml`.

## ğŸ” Security

See `SECURITY.md`. Report privately to `anandhabagavathileos@gmail.com`.

## ğŸ’¬ Support

See `SUPPORT.md` or email `anandhabagavathileos@gmail.com`.

## ğŸ¤ Contributing

See `CONTRIBUTING.md` and `CODE_OF_CONDUCT.md`. PRs welcome!

## ğŸ“œ License

MIT Â© T Anandha Bagavathi


